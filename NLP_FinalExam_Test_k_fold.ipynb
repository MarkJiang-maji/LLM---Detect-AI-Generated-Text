{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 61542,
          "databundleVersionId": 7516023,
          "sourceType": "competition"
        },
        {
          "sourceId": 4737381,
          "sourceType": "datasetVersion",
          "datasetId": 2740486
        },
        {
          "sourceId": 6847931,
          "sourceType": "datasetVersion",
          "datasetId": 3936750
        },
        {
          "sourceId": 6865136,
          "sourceType": "datasetVersion",
          "datasetId": 3945154
        },
        {
          "sourceId": 6888403,
          "sourceType": "datasetVersion",
          "datasetId": 3955358
        },
        {
          "sourceId": 6890527,
          "sourceType": "datasetVersion",
          "datasetId": 3942644
        },
        {
          "sourceId": 6901341,
          "sourceType": "datasetVersion",
          "datasetId": 3960967
        },
        {
          "sourceId": 6920799,
          "sourceType": "datasetVersion",
          "datasetId": 3973977
        },
        {
          "sourceId": 6971638,
          "sourceType": "datasetVersion",
          "datasetId": 3961875
        },
        {
          "sourceId": 6977472,
          "sourceType": "datasetVersion",
          "datasetId": 4005256
        },
        {
          "sourceId": 7018354,
          "sourceType": "datasetVersion",
          "datasetId": 4035516
        },
        {
          "sourceId": 7060310,
          "sourceType": "datasetVersion",
          "datasetId": 3944051
        },
        {
          "sourceId": 7082713,
          "sourceType": "datasetVersion",
          "datasetId": 4039374
        },
        {
          "sourceId": 7254534,
          "sourceType": "datasetVersion",
          "datasetId": 4203616
        },
        {
          "sourceId": 7359107,
          "sourceType": "datasetVersion",
          "datasetId": 4274446
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NLP_FinalExam_Test_k-fold",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "rfRdEG4a_YXY"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llm_detect_ai_generated_text_path = kagglehub.competition_download('llm-detect-ai-generated-text')\n",
        "erenakbulut_sentence_transformers_path = kagglehub.dataset_download('erenakbulut/sentence-transformers')\n",
        "alejopaullier_daigt_external_dataset_path = kagglehub.dataset_download('alejopaullier/daigt-external-dataset')\n",
        "narsil_daigt_misc_path = kagglehub.dataset_download('narsil/daigt-misc')\n",
        "darraghdog_hello_claude_1000_essays_from_anthropic_path = kagglehub.dataset_download('darraghdog/hello-claude-1000-essays-from-anthropic')\n",
        "thedrcat_daigt_proper_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-proper-train-dataset')\n",
        "thedrcat_daigt_external_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-external-train-dataset')\n",
        "kingki19_llm_generated_essay_using_palm_from_google_gen_ai_path = kagglehub.dataset_download('kingki19/llm-generated-essay-using-palm-from-google-gen-ai')\n",
        "carlmcbrideellis_llm_7_prompt_training_dataset_path = kagglehub.dataset_download('carlmcbrideellis/llm-7-prompt-training-dataset')\n",
        "thedrcat_daigt_v2_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-v2-train-dataset')\n",
        "jdragonxherrera_augmented_data_for_llm_detect_ai_generated_text_path = kagglehub.dataset_download('jdragonxherrera/augmented-data-for-llm-detect-ai-generated-text')\n",
        "nbroad_daigt_data_llama_70b_and_falcon180b_path = kagglehub.dataset_download('nbroad/daigt-data-llama-70b-and-falcon180b')\n",
        "carlmcbrideellis_llm_mistral_7b_instruct_texts_path = kagglehub.dataset_download('carlmcbrideellis/llm-mistral-7b-instruct-texts')\n",
        "datafan07_daigt_gemini_pro_8_5k_essays_path = kagglehub.dataset_download('datafan07/daigt-gemini-pro-8-5k-essays')\n",
        "dardodel_4k_mixtral87b_crafted_essays_for_detect_ai_comp_path = kagglehub.dataset_download('dardodel/4k-mixtral87b-crafted-essays-for-detect-ai-comp')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "yOWlLA31_YXZ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# # NOTEBOOK.\n",
        "\n",
        "# llm_detect_ai_generated_text_path = kagglehub.competition_download('llm-detect-ai-generated-text')\n",
        "# erenakbulut_sentence_transformers_path = kagglehub.dataset_download('erenakbulut/sentence-transformers')\n",
        "# alejopaullier_daigt_external_dataset_path = kagglehub.dataset_download('alejopaullier/daigt-external-dataset')\n",
        "# narsil_daigt_misc_path = kagglehub.dataset_download('narsil/daigt-misc')\n",
        "# darraghdog_hello_claude_1000_essays_from_anthropic_path = kagglehub.dataset_download('darraghdog/hello-claude-1000-essays-from-anthropic')\n",
        "# thedrcat_daigt_proper_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-proper-train-dataset')\n",
        "# thedrcat_daigt_external_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-external-train-dataset')\n",
        "# kingki19_llm_generated_essay_using_palm_from_google_gen_ai_path = kagglehub.dataset_download('kingki19/llm-generated-essay-using-palm-from-google-gen-ai')\n",
        "# carlmcbrideellis_llm_7_prompt_training_dataset_path = kagglehub.dataset_download('carlmcbrideellis/llm-7-prompt-training-dataset')\n",
        "# thedrcat_daigt_v2_train_dataset_path = kagglehub.dataset_download('thedrcat/daigt-v2-train-dataset')\n",
        "# jdragonxherrera_augmented_data_for_llm_detect_ai_generated_text_path = kagglehub.dataset_download('jdragonxherrera/augmented-data-for-llm-detect-ai-generated-text')\n",
        "# nbroad_daigt_data_llama_70b_and_falcon180b_path = kagglehub.dataset_download('nbroad/daigt-data-llama-70b-and-falcon180b')\n",
        "# carlmcbrideellis_llm_mistral_7b_instruct_texts_path = kagglehub.dataset_download('carlmcbrideellis/llm-mistral-7b-instruct-texts')\n",
        "# datafan07_daigt_gemini_pro_8_5k_essays_path = kagglehub.dataset_download('datafan07/daigt-gemini-pro-8-5k-essays')\n",
        "# dardodel_4k_mixtral87b_crafted_essays_for_detect_ai_comp_path = kagglehub.dataset_download('dardodel/4k-mixtral87b-crafted-essays-for-detect-ai-comp')\n",
        "\n",
        "# print('Data source import complete.')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:04.210355Z",
          "iopub.execute_input": "2025-05-21T08:02:04.210601Z",
          "iopub.status.idle": "2025-05-21T08:02:04.21745Z",
          "shell.execute_reply.started": "2025-05-21T08:02:04.21058Z",
          "shell.execute_reply": "2025-05-21T08:02:04.216761Z"
        },
        "id": "TTbvDGCW_YXZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "import tensorflow_text as tf_text\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import kagglehub\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:04.218599Z",
          "iopub.execute_input": "2025-05-21T08:02:04.218763Z",
          "iopub.status.idle": "2025-05-21T08:02:22.00487Z",
          "shell.execute_reply.started": "2025-05-21T08:02:04.21875Z",
          "shell.execute_reply": "2025-05-21T08:02:22.004267Z"
        },
        "id": "zosEBivs_YXa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "extra_train = pd.read_csv(\"/kaggle/input/daigt-external-dataset/daigt_external_dataset.csv\")\n",
        "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:22.005521Z",
          "iopub.execute_input": "2025-05-21T08:02:22.006023Z",
          "iopub.status.idle": "2025-05-21T08:02:22.244263Z",
          "shell.execute_reply.started": "2025-05-21T08:02:22.006002Z",
          "shell.execute_reply": "2025-05-21T08:02:22.243633Z"
        },
        "id": "RZHrMYUk_YXa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 5. 建立各種來源的 extra_train_new ───\n",
        "extra_train_1 = pd.read_csv(\"/kaggle/input/llm-mistral-7b-instruct-texts/Mistral7B_CME_v7.csv\").drop(['prompt_id','prompt_name'], axis=1)\n",
        "extra_train_2 = pd.read_csv(\"/kaggle/input/hello-claude-1000-essays-from-anthropic/persuade15_claude_instant1.csv\").drop(['prompt_id','essay_title'],axis=1).rename(columns = {'essay_text': 'text'})\n",
        "extra_train_2['generated'] = 1\n",
        "extra_train_3 = pd.read_csv(\"/kaggle/input/daigt-data-llama-70b-and-falcon180b/llama_falcon_v3.csv\").drop(['prompt_name','model'], axis=1)\n",
        "extra_train_4 = pd.read_csv(\"/kaggle/input/llm-generated-essay-using-palm-from-google-gen-ai/LLM_generated_essay_PaLM.csv\").drop(['id','prompt_id'], axis=1)\n",
        "extra_train_5 = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\").rename(columns = {'label': 'generated'}).drop(['prompt_name','source', 'RDizzl3_seven'], axis=1)\n",
        "extra_train_5 = extra_train_5[extra_train_5['generated'] == 0]\n",
        "extra_train_5 = extra_train_5.sample(n=18149)\n",
        "extra_train_6 = pd.read_csv(\"/kaggle/input/4k-mixtral87b-crafted-essays-for-detect-ai-comp/Mixtral8x7b_4k_essays_for_DetectAIGeneratedTextCompetition.csv\").rename(columns = {'AI_Essay': 'text'}).drop(['model_Raw_output','student_id','prompt_id','index'], axis=1)\n",
        "extra_train_6['generated'] = 1\n",
        "extra_train_7 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv\").rename(columns = {'label': 'generated'}).drop(['source', 'fold'], axis=1)\n",
        "extra_train_8 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_02.csv\").rename(columns = {'label': 'generated'}).drop(['essay_id','source', 'prompt', 'fold'], axis=1)\n",
        "extra_train_9 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_03.csv\").rename(columns = {'label': 'generated'}).drop(['essay_id','source', 'prompt', 'fold'], axis=1)\n",
        "extra_train_10 = pd.read_csv(\"/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv\").rename(columns = {'label': 'generated'}).drop(['essay_id','source', 'prompt', 'fold'], axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:22.245065Z",
          "iopub.execute_input": "2025-05-21T08:02:22.24536Z",
          "iopub.status.idle": "2025-05-21T08:02:34.637536Z",
          "shell.execute_reply.started": "2025-05-21T08:02:22.245332Z",
          "shell.execute_reply": "2025-05-21T08:02:34.636891Z"
        },
        "id": "EAGkg0Pc_YXa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "extra_train_new = pd.concat([extra_train_1, extra_train_2,\n",
        "                             extra_train_3, extra_train_4,\n",
        "                             extra_train_5, extra_train_6,\n",
        "                             extra_train_7, extra_train_8,\n",
        "                             extra_train_9, extra_train_10], axis=0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:34.639892Z",
          "iopub.execute_input": "2025-05-21T08:02:34.640099Z",
          "iopub.status.idle": "2025-05-21T08:02:34.650514Z",
          "shell.execute_reply.started": "2025-05-21T08:02:34.640083Z",
          "shell.execute_reply": "2025-05-21T08:02:34.649893Z"
        },
        "id": "TZmirmsN_YXa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "extra_train_student_generated = extra_train[['text']]\n",
        "extra_train_student_generated['generated'] = 0\n",
        "extra_train_ai_generated = extra_train[['source_text']].rename(columns = {'source_text':'text'})\n",
        "extra_train_ai_generated['generated'] = 1\n",
        "extra_train_nf = pd.concat([extra_train_student_generated, extra_train_ai_generated, extra_train_new], axis=0)\n",
        "extra_train_f = extra_train_nf.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
        "print(f\"最終資料筆數：{len(extra_train_f)}\")\n",
        "extra_train_f"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:34.651147Z",
          "iopub.execute_input": "2025-05-21T08:02:34.651371Z",
          "iopub.status.idle": "2025-05-21T08:02:34.939327Z",
          "shell.execute_reply.started": "2025-05-21T08:02:34.651354Z",
          "shell.execute_reply": "2025-05-21T08:02:34.938659Z"
        },
        "id": "QFalEZUR_YXa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary dataframe with counts of each category\n",
        "count_df = extra_train_f['generated'].value_counts().reset_index()\n",
        "count_df.columns = ['generated', 'count']\n",
        "\n",
        "fig = px.bar(\n",
        "    count_df,\n",
        "    x='generated',\n",
        "    y='count',\n",
        "    title='Distribution of Generated Label',\n",
        "    color=['#2E86AB', '#E84545'],\n",
        "    color_discrete_map=\"identity\"\n",
        ")\n",
        "\n",
        "# Customize layout for value display\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        tickmode='array',\n",
        "        tickvals=[0, 1])\n",
        ")\n",
        "\n",
        "# Display values on top of the bars\n",
        "fig.update_traces(\n",
        "    texttemplate='%{y}',\n",
        "    textposition='outside',\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:34.940027Z",
          "iopub.execute_input": "2025-05-21T08:02:34.940277Z",
          "iopub.status.idle": "2025-05-21T08:02:36.488572Z",
          "shell.execute_reply.started": "2025-05-21T08:02:34.94026Z",
          "shell.execute_reply": "2025-05-21T08:02:36.487938Z"
        },
        "id": "cyI9OX1q_YXb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 8. 簡單視覺化：文字長度分佈 ───\n",
        "print(extra_train_f['generated'].value_counts())\n",
        "\n",
        "# 計算每筆文字的詞數長度\n",
        "extra_train_f['seq_len'] = extra_train_f['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# 分組\n",
        "ai_data = extra_train_f[extra_train_f['generated'] == 1.0]['seq_len']\n",
        "human_data = extra_train_f[extra_train_f['generated'] == 0.0]['seq_len']\n",
        "\n",
        "# 畫圖\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(human_data, bins=30, alpha=0.6, label='Human', color='blue')\n",
        "plt.hist(ai_data, bins=30, alpha=0.6, label='AI-generated', color='orange')\n",
        "\n",
        "plt.title('Text Sequence Length Distribution by Label')\n",
        "plt.xlabel('Sequence Length (word count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:36.489244Z",
          "iopub.execute_input": "2025-05-21T08:02:36.489434Z",
          "iopub.status.idle": "2025-05-21T08:02:38.484327Z",
          "shell.execute_reply.started": "2025-05-21T08:02:36.489419Z",
          "shell.execute_reply": "2025-05-21T08:02:38.483633Z"
        },
        "id": "fiCxMCHv_YXb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 箱型圖\n",
        "extra_train_f['essay_length'] = extra_train_f['text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "extra_train_f['source'] = extra_train_f['generated'].map({0.0: 'Humen', 1.0: 'LLM'})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='source', y='essay_length', data=extra_train_f, palette=['#2980B9', '#E74C3C'])\n",
        "\n",
        "plt.title('Comparison of Essay Lengths by Source')\n",
        "plt.xlabel('Essay Source')\n",
        "plt.ylabel('Essay Length (Word Count)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:38.485078Z",
          "iopub.execute_input": "2025-05-21T08:02:38.485308Z",
          "iopub.status.idle": "2025-05-21T08:02:40.028396Z",
          "shell.execute_reply.started": "2025-05-21T08:02:38.485291Z",
          "shell.execute_reply": "2025-05-21T08:02:40.027671Z"
        },
        "id": "PWdJE-jq_YXb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Human\n",
        "human_texts = extra_train_f[extra_train_f['generated'] == 0.0]['text']\n",
        "\n",
        "all_words = ' '.join(human_texts.astype(str)).lower().split()\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "common_words = word_counts.most_common(20)  # 取前20個常見詞\n",
        "\n",
        "common_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=common_df, x='count', y='word', palette='Blues_d')\n",
        "plt.title('Top 20 Most Common Words in Humen-Written Essays')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Word')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:40.029176Z",
          "iopub.execute_input": "2025-05-21T08:02:40.029387Z",
          "iopub.status.idle": "2025-05-21T08:02:44.521265Z",
          "shell.execute_reply.started": "2025-05-21T08:02:40.029372Z",
          "shell.execute_reply": "2025-05-21T08:02:44.520521Z"
        },
        "id": "1c_XoaOZ_YXb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# AI\n",
        "ai_texts = extra_train_f[extra_train_f['generated'] == 1.0]['text']\n",
        "\n",
        "all_words = ' '.join(ai_texts.astype(str)).lower().split()\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "common_words = word_counts.most_common(20)  # 取前20個常見詞\n",
        "\n",
        "common_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=common_df, x='count', y='word', palette='Blues_d')\n",
        "plt.title('Top 20 Most Common Words in AI-Written Essays')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Word')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:44.521961Z",
          "iopub.execute_input": "2025-05-21T08:02:44.522167Z",
          "iopub.status.idle": "2025-05-21T08:02:49.650967Z",
          "shell.execute_reply.started": "2025-05-21T08:02:44.522144Z",
          "shell.execute_reply": "2025-05-21T08:02:49.650272Z"
        },
        "id": "S8fQsTrj_YXb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 去除stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Human\n",
        "human_texts = extra_train_f[extra_train_f['generated'] == 0.0]['text']\n",
        "all_words = ' '.join(human_texts.astype(str)).lower().split()\n",
        "filtered_words = [word for word in all_words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "word_counts = Counter(filtered_words)\n",
        "common_words = word_counts.most_common(20)  # 前20個\n",
        "\n",
        "common_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=common_df, x='count', y='word', palette='Blues_d')\n",
        "plt.title('Top 20 Most Common Words in Humen-Written Essays (Excluding Stopwords)')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Word')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:02:49.651614Z",
          "iopub.execute_input": "2025-05-21T08:02:49.651899Z",
          "iopub.status.idle": "2025-05-21T08:03:26.591Z",
          "shell.execute_reply.started": "2025-05-21T08:02:49.651881Z",
          "shell.execute_reply": "2025-05-21T08:03:26.590119Z"
        },
        "id": "FCJix_KM_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 去除stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# AI\n",
        "ai_texts = extra_train_f[extra_train_f['generated'] == 1.0]['text']\n",
        "all_words = ' '.join(ai_texts.astype(str)).lower().split()\n",
        "filtered_words = [word for word in all_words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "word_counts = Counter(filtered_words)\n",
        "common_words = word_counts.most_common(20)  # 前20個\n",
        "\n",
        "common_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=common_df, x='count', y='word', palette='Blues_d')\n",
        "plt.title('Top 20 Most Common Words in AI-Written Essays (Excluding Stopwords)')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Word')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:03:26.592086Z",
          "iopub.execute_input": "2025-05-21T08:03:26.592402Z",
          "iopub.status.idle": "2025-05-21T08:04:04.012024Z",
          "shell.execute_reply.started": "2025-05-21T08:03:26.592376Z",
          "shell.execute_reply": "2025-05-21T08:04:04.011204Z"
        },
        "id": "NPU2OajE_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Human\n",
        "human_texts = extra_train_f[extra_train_f['generated'] == 0.0]['text']\n",
        "\n",
        "all_text = ' '.join(human_texts.astype(str)).lower()\n",
        "\n",
        "filtered_words = ' '.join([\n",
        "    word for word in all_text.split()\n",
        "    if word.isalpha() and word not in stop_words\n",
        "])\n",
        "\n",
        "# WordCloud\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(filtered_words)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of Humen-Written Essays (Excluding Stopwords)\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:04:04.015096Z",
          "iopub.execute_input": "2025-05-21T08:04:04.015302Z",
          "iopub.status.idle": "2025-05-21T08:04:28.718147Z",
          "shell.execute_reply.started": "2025-05-21T08:04:04.015287Z",
          "shell.execute_reply": "2025-05-21T08:04:28.717354Z"
        },
        "id": "SJKH6ckZ_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# AI\n",
        "ai_texts = extra_train_f[extra_train_f['generated'] == 1.0]['text']\n",
        "\n",
        "all_text = ' '.join(ai_texts.astype(str)).lower()\n",
        "\n",
        "filtered_words = ' '.join([\n",
        "    word for word in all_text.split()\n",
        "    if word.isalpha() and word not in stop_words\n",
        "])\n",
        "\n",
        "# WordCloud\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color='white').generate(filtered_words)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of AI-Written Essays (Excluding Stopwords)\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:04:28.718886Z",
          "iopub.execute_input": "2025-05-21T08:04:28.719104Z",
          "iopub.status.idle": "2025-05-21T08:04:52.630318Z",
          "shell.execute_reply.started": "2025-05-21T08:04:28.719088Z",
          "shell.execute_reply": "2025-05-21T08:04:52.629567Z"
        },
        "id": "SnKyWpQR_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 標點符號使用頻率分析\n",
        "punctuation_marks = ['.', ',', '!', '?']\n",
        "\n",
        "for mark in punctuation_marks:\n",
        "    extra_train_f[f'punct_{mark}'] = extra_train_f['text'].apply(\n",
        "        lambda x: str(x).count(mark) / len(str(x)) if len(str(x)) > 0 else 0\n",
        "    )\n",
        "\n",
        "extra_train_f['source'] = extra_train_f['generated'].map({0.0: 'Human', 1.0: 'LLM'})\n",
        "\n",
        "# boxplot\n",
        "plt.figure(figsize=(14, 10))\n",
        "for i, mark in enumerate(punctuation_marks):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    sns.boxplot(data=extra_train_f, x='source', y=f'punct_{mark}', palette='Set2')\n",
        "    plt.title(f\"Usage of '{mark}' by Source\")\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('Frequency per Character')\n",
        "\n",
        "plt.suptitle(\"Punctuation Usage Frequency: Human vs LLM\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:04:52.631038Z",
          "iopub.execute_input": "2025-05-21T08:04:52.631323Z",
          "iopub.status.idle": "2025-05-21T08:04:53.860364Z",
          "shell.execute_reply.started": "2025-05-21T08:04:52.631298Z",
          "shell.execute_reply": "2025-05-21T08:04:53.85958Z"
        },
        "id": "khTaIV1j_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "human_texts = extra_train_f[extra_train_f['generated'] == 0.0]['text'].astype(str)\n",
        "ai_texts = extra_train_f[extra_train_f['generated'] == 1.0]['text'].astype(str)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "human_tfidf = vectorizer.fit_transform(human_texts)\n",
        "ai_tfidf = vectorizer.fit_transform(ai_texts)\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# 平均TF-IDF權重\n",
        "human_avg = human_tfidf.mean(axis=0).A1\n",
        "ai_avg = ai_tfidf.mean(axis=0).A1\n",
        "\n",
        "# top 10\n",
        "human_top10 = pd.DataFrame({'word': words, 'score': human_avg}).nlargest(10, 'score')\n",
        "ai_top10 = pd.DataFrame({'word': words, 'score': ai_avg}).nlargest(10, 'score')\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=human_top10, x='score', y='word', palette='Blues_d')\n",
        "plt.title(\"Top 10 TF-IDF Keywords in Human Essays\")\n",
        "plt.xlabel(\"TF-IDF Score\")\n",
        "plt.ylabel(\"Keyword\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=ai_top10, x='score', y='word', palette='Oranges_d')\n",
        "plt.title(\"Top 10 TF-IDF Keywords in LLM Essays\")\n",
        "plt.xlabel(\"TF-IDF Score\")\n",
        "plt.ylabel(\"Keyword\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:04:53.86115Z",
          "iopub.execute_input": "2025-05-21T08:04:53.861357Z",
          "iopub.status.idle": "2025-05-21T08:05:07.73193Z",
          "shell.execute_reply.started": "2025-05-21T08:04:53.861342Z",
          "shell.execute_reply": "2025-05-21T08:05:07.73109Z"
        },
        "id": "f0ROq1YJ_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "sample_df = extra_train_f[['text', 'generated']].sample(n=3000, random_state=42).copy()\n",
        "\n",
        "# 分析情感分數\n",
        "sample_df['sentiment'] = sample_df['text'].apply(lambda x: sid.polarity_scores(str(x))['compound'])\n",
        "\n",
        "sample_df['source'] = sample_df['generated'].map({0: 'Human', 1: 'LLM'})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=sample_df, x='source', y='sentiment', palette='coolwarm')\n",
        "plt.title('Sentiment Score Distribution: Human vs LLM')\n",
        "plt.xlabel('Essay Source')\n",
        "plt.ylabel('Compound Sentiment Score (-1 to 1)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:07.732846Z",
          "iopub.execute_input": "2025-05-21T08:05:07.733176Z",
          "iopub.status.idle": "2025-05-21T08:05:48.285498Z",
          "shell.execute_reply.started": "2025-05-21T08:05:07.733159Z",
          "shell.execute_reply": "2025-05-21T08:05:48.284558Z"
        },
        "id": "o3SeuSee_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = extra_train_f\n",
        "train_final"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:48.286371Z",
          "iopub.execute_input": "2025-05-21T08:05:48.286616Z",
          "iopub.status.idle": "2025-05-21T08:05:48.300305Z",
          "shell.execute_reply.started": "2025-05-21T08:05:48.286597Z",
          "shell.execute_reply": "2025-05-21T08:05:48.299496Z"
        },
        "id": "1pULSpXb_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 8. 簡單視覺化：文字長度分佈 ───\n",
        "sequence_lengths = train_final['text'].apply(lambda x: len(x.split()))\n",
        "plt.hist(sequence_lengths, bins=30, alpha=0.75, color='blue')\n",
        "plt.title('Distribution of Text Sequence Lengths')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:48.301092Z",
          "iopub.execute_input": "2025-05-21T08:05:48.301402Z",
          "iopub.status.idle": "2025-05-21T08:05:49.859166Z",
          "shell.execute_reply.started": "2025-05-21T08:05:48.301375Z",
          "shell.execute_reply": "2025-05-21T08:05:49.85847Z"
        },
        "id": "VVOJdJ7I_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_final['generated'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:49.860003Z",
          "iopub.execute_input": "2025-05-21T08:05:49.860282Z",
          "iopub.status.idle": "2025-05-21T08:05:49.865881Z",
          "shell.execute_reply.started": "2025-05-21T08:05:49.860257Z",
          "shell.execute_reply": "2025-05-21T08:05:49.865194Z"
        },
        "id": "7ZOOA6sb_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(train_final, test_size=0.3, random_state=222)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_df['text'].values, train_df['generated'].values)\n",
        ").batch(batch_size)\n",
        "\n",
        "raw_val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_df['text'].values, val_df['generated'].values)\n",
        ").batch(batch_size)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:49.866728Z",
          "iopub.execute_input": "2025-05-21T08:05:49.867014Z",
          "iopub.status.idle": "2025-05-21T08:05:51.483956Z",
          "shell.execute_reply.started": "2025-05-21T08:05:49.866991Z",
          "shell.execute_reply": "2025-05-21T08:05:51.483019Z"
        },
        "id": "53Bch3aS_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 75000\n",
        "embedding_dim = 64\n",
        "sequence_length = 512\n",
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_features,\n",
        "    ngrams = (3,5),\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:05:51.484894Z",
          "iopub.execute_input": "2025-05-21T08:05:51.485191Z",
          "iopub.status.idle": "2025-05-21T08:07:49.271007Z",
          "shell.execute_reply.started": "2025-05-21T08:05:51.485168Z",
          "shell.execute_reply": "2025-05-21T08:07:49.270474Z"
        },
        "id": "YoNjRBH9_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_handcrafted_features(texts):\n",
        "    features = []\n",
        "    for t in texts:\n",
        "        word_count = len(t.split())\n",
        "        capital_ratio = sum(1 for c in t if c.isupper()) / (len(t) + 1)\n",
        "        punctuation_ratio = sum(t.count(p) for p in ['.', ',', '?', '!']) / (len(t) + 1)\n",
        "        features.append([word_count, capital_ratio, punctuation_ratio])\n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "# 抽特徵\n",
        "train_feats = extract_handcrafted_features(train_df['text'].tolist())\n",
        "val_feats = extract_handcrafted_features(val_df['text'].tolist())\n",
        "\n",
        "# 轉成 Tensor\n",
        "train_feat_tensor = tf.convert_to_tensor(train_feats, dtype=tf.float32)\n",
        "val_feat_tensor = tf.convert_to_tensor(val_feats, dtype=tf.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:07:49.271816Z",
          "iopub.execute_input": "2025-05-21T08:07:49.272015Z",
          "iopub.status.idle": "2025-05-21T08:07:55.354348Z",
          "shell.execute_reply.started": "2025-05-21T08:07:49.272001Z",
          "shell.execute_reply": "2025-05-21T08:07:55.35381Z"
        },
        "id": "fgzT-CDY_YXc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 結合 text 和統計特徵作為雙輸入\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((\n",
        "    (train_df['text'].values, train_feat_tensor),\n",
        "    train_df['generated'].values\n",
        "))\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((\n",
        "    (val_df['text'].values, val_feat_tensor),\n",
        "    val_df['generated'].values\n",
        "))\n",
        "\n",
        "# 定義 mapping function\n",
        "def vectorize_with_features(inputs, label):\n",
        "    text, feats = inputs\n",
        "    return (vectorize_layer(text), feats), label\n",
        "\n",
        "train_ds = train_ds.map(vectorize_with_features).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(vectorize_with_features).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:07:55.355111Z",
          "iopub.execute_input": "2025-05-21T08:07:55.355316Z",
          "iopub.status.idle": "2025-05-21T08:07:55.777712Z",
          "shell.execute_reply.started": "2025-05-21T08:07:55.355301Z",
          "shell.execute_reply": "2025-05-21T08:07:55.777057Z"
        },
        "id": "ilOd39U0_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_features,\n",
        "    ngrams=(3,5),\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,  # e.g., 512\n",
        "    pad_to_max_tokens=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:07:55.778335Z",
          "iopub.execute_input": "2025-05-21T08:07:55.778531Z",
          "iopub.status.idle": "2025-05-21T08:07:55.785831Z",
          "shell.execute_reply.started": "2025-05-21T08:07:55.778516Z",
          "shell.execute_reply": "2025-05-21T08:07:55.785082Z"
        },
        "id": "64mKkP96_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:07:55.786643Z",
          "iopub.execute_input": "2025-05-21T08:07:55.786905Z",
          "iopub.status.idle": "2025-05-21T08:07:55.802444Z",
          "shell.execute_reply.started": "2025-05-21T08:07:55.786884Z",
          "shell.execute_reply": "2025-05-21T08:07:55.801938Z"
        },
        "id": "o_ygPwd7_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1) 重新定義純 Transformer Builder，呼叫時帶 training=False\n",
        "def build_pure_transformer_model_nf(max_features, embedding_dim, sequence_length,\n",
        "                                    num_heads=2, ff_dim=64, num_blocks=2, rate=0.1):\n",
        "    inputs = Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "    x = Embedding(max_features, embedding_dim)(inputs)\n",
        "    for _ in range(num_blocks):\n",
        "        block = TransformerBlock(embedding_dim, num_heads, ff_dim, rate)\n",
        "        x = block(x)            # <-- 不要寫 training=False\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "    return Model(inputs, outputs, name=\"PureTransformer\")\n",
        "\n",
        "# 2) 準備資料\n",
        "texts = train_final['text'].astype(str).values\n",
        "labels = train_final['generated'].astype(int).values\n",
        "\n",
        "n_splits    = 5\n",
        "batch_size  = 32\n",
        "epochs      = 5\n",
        "kf          = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "acc_per_fold  = []\n",
        "loss_per_fold = []\n",
        "\n",
        "fold_no = 1\n",
        "\n",
        "vectorize_layer.adapt(\n",
        "    tf.data.Dataset.from_tensor_slices(texts).batch(batch_size)\n",
        ")\n",
        "\n",
        "for train_idx, val_idx in kf.split(texts, labels):\n",
        "    X_train, X_val = texts[train_idx], texts[val_idx]\n",
        "    y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "    raw_train_ds = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices((X_train, y_train))\n",
        "          .shuffle(buffer_size=len(X_train), seed=42)   # <-- shuffle!\n",
        "          .batch(batch_size)\n",
        "    )\n",
        "    raw_val_ds = (\n",
        "        tf.data.Dataset\n",
        "          .from_tensor_slices((X_val, y_val))\n",
        "          .batch(batch_size)\n",
        "    )\n",
        "\n",
        "    train_ds = raw_train_ds.map(vectorize_text).prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds   = raw_val_ds.map(vectorize_text).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    model_k = build_pure_transformer_model_nf(max_features, embedding_dim, sequence_length)\n",
        "    model_k.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "    history_kfold = model_k.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,      # 可以多跑幾個 epoch\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5) 評估\n",
        "    scores = model_k.evaluate(val_ds, verbose=0)\n",
        "    print(f'Fold {fold_no} — Loss: {scores[0]:.4f}, Acc: {scores[1]:.4f}')\n",
        "    loss_per_fold.append(scores[0])\n",
        "    acc_per_fold.append(scores[1])\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# 6) 印出平均與標準差\n",
        "print('\\n===== K-Fold 結果 =====')\n",
        "print(f'Average loss: {np.mean(loss_per_fold):.4f} ± {np.std(loss_per_fold):.4f}')\n",
        "print(f'Average acc : {np.mean(acc_per_fold):.4f} ± {np.std(acc_per_fold):.4f}')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:10:56.211804Z",
          "iopub.execute_input": "2025-05-21T08:10:56.212402Z",
          "iopub.status.idle": "2025-05-21T08:35:01.309691Z",
          "shell.execute_reply.started": "2025-05-21T08:10:56.212381Z",
          "shell.execute_reply": "2025-05-21T08:35:01.308748Z"
        },
        "id": "ubaImvQi_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test['text'].values\n",
        "vectorized_test_text = vectorize_layer(test_text)\n",
        "test_feats = extract_handcrafted_features(test_text)\n",
        "predictions = model_k.predict([vectorized_test_text])\n",
        "test['generated'] = predictions\n",
        "print(test[['text', 'generated']])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:21.735217Z",
          "iopub.execute_input": "2025-05-21T08:36:21.735701Z",
          "iopub.status.idle": "2025-05-21T08:36:23.061195Z",
          "shell.execute_reply.started": "2025-05-21T08:36:21.735682Z",
          "shell.execute_reply": "2025-05-21T08:36:23.060516Z"
        },
        "id": "qSGbt2Zs_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test[['id', 'generated']].to_csv(\"submission.csv\")\n",
        "# test[['id', 'generated']].to_csv(\"submission.csv\", index=false)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:25.258209Z",
          "iopub.execute_input": "2025-05-21T08:36:25.258487Z",
          "iopub.status.idle": "2025-05-21T08:36:25.269617Z",
          "shell.execute_reply.started": "2025-05-21T08:36:25.258455Z",
          "shell.execute_reply": "2025-05-21T08:36:25.269039Z"
        },
        "id": "QkFno46__YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:25.556963Z",
          "iopub.execute_input": "2025-05-21T08:36:25.557884Z",
          "iopub.status.idle": "2025-05-21T08:36:25.567136Z",
          "shell.execute_reply.started": "2025-05-21T08:36:25.557856Z",
          "shell.execute_reply": "2025-05-21T08:36:25.566418Z"
        },
        "id": "OPaTNt0D_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub[\"generated\"] = predictions.squeeze()   # 或 predictions[:, 0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:25.746399Z",
          "iopub.execute_input": "2025-05-21T08:36:25.74664Z",
          "iopub.status.idle": "2025-05-21T08:36:25.750231Z",
          "shell.execute_reply.started": "2025-05-21T08:36:25.746622Z",
          "shell.execute_reply": "2025-05-21T08:36:25.749645Z"
        },
        "id": "gxKESnox_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:27.724802Z",
          "iopub.execute_input": "2025-05-21T08:36:27.72511Z",
          "iopub.status.idle": "2025-05-21T08:36:27.730046Z",
          "shell.execute_reply.started": "2025-05-21T08:36:27.725093Z",
          "shell.execute_reply": "2025-05-21T08:36:27.729476Z"
        },
        "id": "9igTgmxS_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_sub.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:27.875222Z",
          "iopub.execute_input": "2025-05-21T08:36:27.875457Z",
          "iopub.status.idle": "2025-05-21T08:36:27.881443Z",
          "shell.execute_reply.started": "2025-05-21T08:36:27.875442Z",
          "shell.execute_reply": "2025-05-21T08:36:27.880816Z"
        },
        "id": "bmHT3KiD_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-21T08:36:28.223922Z",
          "iopub.execute_input": "2025-05-21T08:36:28.224205Z",
          "iopub.status.idle": "2025-05-21T08:36:28.230214Z",
          "shell.execute_reply.started": "2025-05-21T08:36:28.224188Z",
          "shell.execute_reply": "2025-05-21T08:36:28.229362Z"
        },
        "id": "hcvA99_b_YXd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eWqPvdvJ_YXd"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}